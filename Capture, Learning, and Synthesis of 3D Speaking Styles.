Capture, Learning, and Synthesis of 3D Speaking Styles.
D. Cudeiro*, T. Bolkart*, C. Laidlaw, A. Ranjan, M. J. Black
Computer Vision and Pattern Recognition (CVPR), 2019

For the Abstract:
Audio-driven 3D facial animation has been widely explored, but achieving realistic, human-like performance is
still unsolved. This is due to the lack of available 3D
datasets, models, and standard evaluation metrics. To address this, we introduce a unique 4D face dataset with about
29 minutes of 4D scans captured at 60 fps and synchronized audio from 12 speakers. We then train a neural network on our dataset that factors identity from facial motion. 

It shows the authors take steps:
1. collect training dataset, the temporal sequence of real faces (3D) delivering speaking
2. take trimmed dataset into a designed NN network to get a NN model
3. verification on the generated model.
